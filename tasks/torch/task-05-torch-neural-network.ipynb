{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Torch Sample Net Class\n",
    "Pytorch 에서는 Neural Network Module 클래스를 제공해준다. Module은 파라미터 분석, GPU, Exporting, Loading 등의 편리한 기능을 제공한다.\n",
    "\n",
    "특히 forward 부분만 제공해도 back propagation 부분을 자동으로 제공해주는데, Autograd 기능을 기본적으로 사용하기 때문에 사용하기 쉽다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 128)\n",
    "        self.fc2 = nn.Linear(128, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x)) # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        \n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1517,  0.2703, -0.0641],\n",
      "          [-0.0864, -0.0806, -0.1732],\n",
      "          [ 0.0268,  0.0227,  0.3258]]],\n",
      "\n",
      "\n",
      "        [[[-0.1264, -0.2172, -0.1967],\n",
      "          [ 0.2041,  0.2621,  0.1639],\n",
      "          [ 0.1060, -0.2884,  0.1346]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0691,  0.0900,  0.1056],\n",
      "          [-0.0724, -0.1461,  0.2538],\n",
      "          [-0.1159, -0.2514, -0.2045]]],\n",
      "\n",
      "\n",
      "        [[[-0.0328,  0.3147,  0.1305],\n",
      "          [ 0.0503, -0.1428,  0.2043],\n",
      "          [-0.1962,  0.0285,  0.2514]]],\n",
      "\n",
      "\n",
      "        [[[-0.0648,  0.3001, -0.0829],\n",
      "          [-0.0422,  0.2867, -0.2773],\n",
      "          [-0.2023, -0.1014, -0.2773]]],\n",
      "\n",
      "\n",
      "        [[[-0.1163,  0.2336, -0.0223],\n",
      "          [-0.1994,  0.1215,  0.0291],\n",
      "          [ 0.3244, -0.0400, -0.2256]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "\n",
    "print(len(params))\n",
    "print(params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0932,  0.0126, -0.0428,  0.1333, -0.0101,  0.0387, -0.0174,  0.0377,\n",
      "         -0.1365,  0.0199]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward opartion.\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4171, grad_fn=<MseLossBackward>)\n",
      "<MseLossBackward object at 0x11e1fd810>\n",
      "<AddmmBackward object at 0x11dd5aed0>\n",
      "<AccumulateGrad object at 0x11e1fd810>\n"
     ]
    }
   ],
   "source": [
    "# Loss \n",
    "output = net(input)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "\n",
    "print(loss)\n",
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "conv1.bias.grad after backward\n",
      "tensor([[[[-0.0072,  0.0018, -0.0066],\n",
      "          [ 0.0045,  0.0120, -0.0136],\n",
      "          [-0.0083,  0.0027,  0.0231]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0267,  0.0181, -0.0174],\n",
      "          [-0.0102, -0.0016, -0.0152],\n",
      "          [-0.0170,  0.0095, -0.0092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0130,  0.0023, -0.0120],\n",
      "          [-0.0120,  0.0069,  0.0037],\n",
      "          [-0.0149, -0.0054, -0.0069]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0009,  0.0162, -0.0086],\n",
      "          [ 0.0080,  0.0076,  0.0048],\n",
      "          [-0.0159,  0.0240,  0.0225]]],\n",
      "\n",
      "\n",
      "        [[[-0.0024, -0.0117,  0.0133],\n",
      "          [-0.0071, -0.0018, -0.0130],\n",
      "          [-0.0137, -0.0162, -0.0090]]],\n",
      "\n",
      "\n",
      "        [[[-0.0214,  0.0122, -0.0009],\n",
      "          [ 0.0068,  0.0060,  0.0051],\n",
      "          [-0.0023,  0.0246, -0.0019]]]])\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation\n",
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.weight.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
