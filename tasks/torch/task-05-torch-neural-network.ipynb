{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Torch Sample Net Class\n",
    "Pytorch 에서는 Neural Network Module 클래스를 제공해준다. Module은 파라미터 분석, GPU, Exporting, Loading 등의 편리한 기능을 제공한다.\n",
    "\n",
    "특히 forward 부분만 제공해도 back propagation 부분을 자동으로 제공해주는데, Autograd 기능을 기본적으로 사용하기 때문에 사용하기 쉽다.\n",
    "\n",
    "**참고자료**\n",
    "* https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 128)\n",
    "        self.fc2 = nn.Linear(128, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x)) # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        print(\"x:\", x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        \n",
    "        return num_features\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 0.0437, -0.1248,  0.0678,  0.1429,  0.0958,  0.0628,  0.0539, -0.0716,\n",
      "         -0.0768, -0.0179]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.0437, -0.1248,  0.0678,  0.1429,  0.0958,  0.0628,  0.0539, -0.0716,\n",
      "         -0.0768, -0.0179]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward opartion.\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4171, grad_fn=<MseLossBackward>)\n",
      "<MseLossBackward object at 0x11e1fd810>\n",
      "<AddmmBackward object at 0x11dd5aed0>\n",
      "<AccumulateGrad object at 0x11e1fd810>\n"
     ]
    }
   ],
   "source": [
    "# Loss \n",
    "output = net(input)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "\n",
    "print(loss)\n",
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "conv1.bias.grad after backward\n",
      "tensor([[[[-0.0072,  0.0018, -0.0066],\n",
      "          [ 0.0045,  0.0120, -0.0136],\n",
      "          [-0.0083,  0.0027,  0.0231]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0267,  0.0181, -0.0174],\n",
      "          [-0.0102, -0.0016, -0.0152],\n",
      "          [-0.0170,  0.0095, -0.0092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0130,  0.0023, -0.0120],\n",
      "          [-0.0120,  0.0069,  0.0037],\n",
      "          [-0.0149, -0.0054, -0.0069]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0009,  0.0162, -0.0086],\n",
      "          [ 0.0080,  0.0076,  0.0048],\n",
      "          [-0.0159,  0.0240,  0.0225]]],\n",
      "\n",
      "\n",
      "        [[[-0.0024, -0.0117,  0.0133],\n",
      "          [-0.0071, -0.0018, -0.0130],\n",
      "          [-0.0137, -0.0162, -0.0090]]],\n",
      "\n",
      "\n",
      "        [[[-0.0214,  0.0122, -0.0009],\n",
      "          [ 0.0068,  0.0060,  0.0051],\n",
      "          [-0.0023,  0.0246, -0.0019]]]])\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation\n",
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.weight.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
